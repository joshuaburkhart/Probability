---
title: "MATH 630 Midterm"
author: "Joshua Burkhart"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
---
```{r global_options, echo=FALSE, include=FALSE, error=FALSE}
knitr::opts_chunk$set(fig.path = "Figs/",
                      message = FALSE,
                      warning = FALSE,
                      include = TRUE,
                      echo = TRUE,
                      error = TRUE,
                      fig.width = 11,
                      comment = NA)
```

Midterm: Simple Linear Regression
=================================

Overview
--------
```{r, echo=FALSE, include=FALSE}
library(plyr) #this must be loaded before dplyr 
library(dplyr)
library(ggplot2)
library(broom)
library(knitr)
library(magrittr)
```

```{r, echo=FALSE, include=FALSE}
library(Lahman)
?Lahman
```

```{r}
Teams <- Lahman::Teams
Salaries <- Lahman::Salaries
```

HLO Lahman
----------

```{r, results="hide"}
str(Teams)
str(Salaries)
glimpse(Teams)
glimpse(Salaries)
head(Teams)
head(Salaries)
tail(Teams)
tail(Salaries)
names(Teams)
names(Salaries)
ncol(Teams)
ncol(Salaries)
length(Teams)
length(Salaries)
head(rownames(Teams))
head(rownames(Salaries))
dim(Teams)
dim(Salaries)
nrow(Teams)
nrow(Salaries)
summary(Teams)
summary(Salaries)
?Teams
?Salaries
```

Are they data.frames, matrices, vectors, lists?

> data.frames  

What is the unit of analysis in the dataset?

> Teams: One row per team-year.  
  Salaries: One row per player-team-year.  

How many variables/columns?

> Teams: 48  
  Salaries: 5  

How many rows/observations?

> Teams: 2775  
  Salaries: 24758  

Find the variables for games won, team, year, and salary.

> W: Wins  
  teamID: Team; a factor  
  yearID: Year  
  salary: Salary  

Which variables are continuous?

> In theory, salary could be continuous but in practice, salary looks like it's rounded to the nearest thousand.  

Which variables are discrete?

> W  
  teamID  
  yearID  
  salary  

Which variables are categorical?

> teamID  

How many levels do they have?

> 149  

What about missing data for any variables?

```{r, results="hide"}
unique(is.na(Teams))
unique(is.na(Salaries))
```

> Teams: Missing data in several columns  
  Salaries: No missing data reported  
  
Data wrangling in dplyr
-----------------------

Create a new dataset that includes total yearly payroll for each team in the Salaries dataframe.

```{r}
typ <- Salaries %>% group_by(yearID,teamID) %>% summarise(payroll = sum(salary))
head(typ)
```

Add this payroll column to the Teams dataframe.

```{r}
teams_pay <- inner_join(Teams,typ,c("yearID","teamID"))
head(teams_pay)
```

We'll focus on the years 2000 - 2014. Use dplyr to filter() the dataset you created with the Teams data plus the payroll column for just those years.

```{r}
recent_tpay <- filter(teams_pay, yearID >= 2000, yearID <= 2014)
```

Gift
```{r}
bat_stats <-
  battingStats(data = Lahman::Batting,
               idvars = c("playerID",
                          "yearID",
                          "stint",
                          "teamID",
                          "lgID"), cbind = TRUE)
```

Write a dplyr expression to create a new dataframe that contains means for each of these three new variables for each team and year from 2000 - 2014 (rather than for each player). 

```{r}
bat_avgs <- bat_stats %>%
  filter(yearID >= 2000, yearID <= 2014) %>% 
  group_by(yearID,teamID) %>% 
  summarise(ob_perc = mean(OBP,na.rm = TRUE),
            slug_perc = mean(SlugPct,na.rm = TRUE),
            ops = mean(OPS,na.rm = TRUE))
head(bat_avgs)
```

Adds these new batting statistic columns to your current dataframe

```{r}
# warning seems ok based on http://goo.gl/9QH3fo
teams_bat <- inner_join(bat_avgs,recent_tpay,c("yearID","teamID"))
head(teams_bat)
```

Univariate EDA (+ more wrangling)
---------------------------------

```{r}
teams_bat %>%
  group_by(teamID) %>%
  tally() %>% arrange(n) %>%
  print(n = 33) 
```

How many teams are there?

> 33  

Which teams have data for the least number of seasons?

> MIA, ANA, and MON 

Which have the most seasons?

> ARI,ATL,BAL,BOS,CHA,CHN,CIN,CLE,COL,  
  DET,HOU,KCA,LAN,MIL,MIN,NYA,NYN,OAK,  
  PHI,PIT,SDN,SEA,SFN,SLN,TBA,TEX,TOR all have 15  

```{r}
teams_bat <- teams_bat %>%
  filter(!(teamID %in% c("ANA", "MIA", "MON"))) # you should understand what this does
```

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(G) %>%
  summarise_each(funs(min,max,mean,median))
```

Is there a lot of variability in number of games played per season across teams?

> No  

What is the range of games played by teams per season?

> 2 (163 - 161)  

Number of games won

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(W) %>%
  summarise_each(funs(min,max)) %>% head()
```

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(W) %>%
  summarise_each(funs(min,max)) %>% ggplot() +
  geom_smooth(aes(x=yearID,y=min),color="orange") +
  geom_smooth(aes(x=yearID,y=max),color="purple")
```

> In the figure above, we see that the maximum and minimum number
  of wins slightly decreases and increases, respectively. This 
  shows that, as a whole, the league is becoming more heterogeneous
  and competative.  

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(W) %>%
  summarise_each(funs(min,max)) %>% ggplot() +
  geom_violin(aes(x=factor("Min"),y=min),fill="orange") +
  geom_violin(aes(x=factor("Max"),y=max),fill="purple") +
  geom_boxplot(aes(x=factor("Min"),y=min),width=0.1) +
  geom_boxplot(aes(x=factor("Max"),y=max),width=0.1) +
  ylab("Wins") + 
  xlab("Max/Min")
```

Mean on-base percentage

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(ob_perc) %>%
  summarise_each(funs(mean)) %>% head()
```

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(ob_perc,slug_perc) %>%
  summarise_each(funs(mean)) %>% ggplot() +
  geom_smooth(aes(x=yearID,y=ob_perc),color="green")
```

> In the plot above we see that on-base percentage has decreased
  slightly in recent years.

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(ob_perc,slug_perc) %>%
  summarise_each(funs(mean)) %>% ggplot() +
  geom_violin(aes(x=factor("On-Base Pct."),y=ob_perc),fill="green") +
  geom_boxplot(aes(x=factor("On-Base Pct."),y=ob_perc),width=0.1) +
  ylab("On-Base Pct.") +
  xlab("")
```

Mean slugging percentage

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(slug_perc) %>%
  summarise_each(funs(mean)) %>% head()
```

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(ob_perc,slug_perc) %>%
  summarise_each(funs(mean)) %>% ggplot() +
  geom_smooth(aes(x=yearID,y=slug_perc),color="red")
```

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(ob_perc,slug_perc) %>%
  summarise_each(funs(mean)) %>% ggplot() +
  geom_violin(aes(x=factor("Slugging Pct."),y=slug_perc),fill="red") +
  geom_boxplot(aes(x=factor("Slugging Pct."),y=slug_perc),width=0.1) +
  ylab("Slugging Pct.") +
  xlab("")
```

Mean on-base percentage + slugging

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(ob_perc,slug_perc) %>%
  summarise_each(funs(mean)) %>% head()
```

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(ob_perc,slug_perc) %>%
  summarise_each(funs(mean)) %>% ggplot() +
  geom_smooth(aes(x=yearID,y=ob_perc),color="green") +
  geom_smooth(aes(x=yearID,y=slug_perc),color="red") +
  geom_smooth(aes(x=yearID,y=(slug_perc - ob_perc)),color="blue")
```

```{r}
teams_bat %>%
  group_by(yearID) %>%
  select(ob_perc,slug_perc) %>%
  summarise_each(funs(mean)) %>% ggplot() +
  geom_violin(aes(x=factor("On-Base Pct."),y=ob_perc),fill="green") + 
  geom_violin(aes(x=factor("Slugging Pct."),y=slug_perc),fill="red") +
  geom_boxplot(aes(x=factor("On-Base Pct."),y=ob_perc),width=0.1) +
  geom_boxplot(aes(x=factor("Slugging Pct."),y=slug_perc),width=0.1) +
  ylab("Pct.") +
  xlab("On-Base / Slugging")
```

Bivariate EDA (+ even more wrangling)
-------------------------------------

Use ggplot2 to create a scatterplot showing payroll (x-axis) and wins (y-axis) across all time periods and teams.

```{r}
teams_pay %>% 
  select(payroll,W) %>%
  ggplot() +
  geom_point(aes(x=payroll/1000,y=W)) +
  geom_smooth(aes(x=payroll/1000,y=W),method="lm")
```

One variable we are not accounting for in this scatterplot is year. It is possible that payrolls increase from season to season. Check this out using the same ggplot code you just used above, but make this plot with year on the x-axis and payroll/1000 on the y-axis.

```{r}
teams_pay %>% 
  select(yearID,payroll) %>% 
  ggplot() +
  geom_point(aes(x=yearID,y=payroll/1000)) + 
  geom_smooth(aes(x=yearID,y=payroll/1000),method="lm")
```

A scatterplot may not be the best way to look at this pattern, since year is a discrete variable. So also try making boxplots stratified by yearID.

```{r}
teams_pay %>%
  select(yearID,payroll) %>% 
  ggplot() +
  geom_boxplot(aes(x=factor(yearID),y=payroll/1000))
```

Create new variables for the average payroll and the standard deviation of payrolls each year across teams and add them to your dataframe.

```{r}
teams_bat <- teams_bat %>% group_by(yearID) %>% 
  mutate(avg_pay = mean(payroll),
         std_pay = sd(payroll))
```

Add another variable to your dataset that is the z-score for each team for each year.

```{r}
teams_bat <- teams_bat %>% group_by(yearID) %>% 
  mutate(z_pay = (payroll - avg_pay) / std_pay)
```

Make a scatterplot in ggplot with year on the x-axis and payroll z-scores on the y-axis and two geoms: geom_point() and geom_smooth(method = "lm").

```{r}
teams_bat %>% 
  select(yearID,z_pay) %>% 
  ggplot() +
  geom_point(aes(x=yearID,y=z_pay)) + 
  geom_smooth(aes(x=yearID,y=z_pay),method="lm")
```

What do you see? 

> I see a scatter plot centered about y = 0 with a
  z_pay spread that looks similar across year. There
  don't appear to be obvious outliers in this data 
  except perhaps for the Yankees.

How is this plot different from the previous one with payroll/1000 on the y-axis (from #12)?

> The other plot showed payroll/1000 increasing
with year. This plot shows z_pay steady across year
(which, of course, makes perfect sense as it's 
relative to each year's mean payroll)

Make a new scatterplot (minus geom_smooth) in ggplot with year on the x-axis and payroll z-scores on the y-axis. This time, add an additional aesthetic to colour the points in the scatterplot with a different color for each teamID, and an additional geom called geom_line().

```{r}
teams_bat %>%
  select(yearID,z_pay,teamID) %>% 
  ggplot() +     
  geom_point(aes(x=yearID,y=z_pay)) + 
  geom_line(aes(x=yearID,y=z_pay,color=factor(teamID)))
```

What do you see? 

> I see points indicating team payroll by year 
connected by colored lines indicating teamID.

What is not surprising here?

> Teams tend to move only slightly (relative 
to each other) each year. Teams at the top 
tend to stay toward the top, teams on the bottom tend
to stay toward the bottom.

Use dplyr to create a new dataset ... that includes two new variables: average payroll z-score and average number of wins. Both averages should be calculated for each team across all seasons.

```{r}
teams_anl <- teams_bat %>%
  group_by(teamID) %>%
  summarise(avg_z_pay = mean(z_pay,na.rm=TRUE),
  avg_w_cnt = mean(W,na.rm=TRUE))
```

What will be the mean and standard deviation of this new variable across the 30 teams?

> mean: This will be the mean distance of the annual payroll from the annual mean payroll across teams.  

> sd: This will be the spread of the distance
of the annual payroll from the annual mean
payroll across teams.  

That is, is your new average payroll z-score also a z-score?

> No.

Are you surprised?

> Yes!

Why or why not?

> I thought about a z-score has having a unit so it made sense to me that an average of z-scores keeps its z-scoreness.

Now create a scatterplot to see the association between average payroll z-scores (x-axis) and average number of wins (y-axis).

```{r}
teams_anl %>% ggplot() +
  geom_point(aes(x=avg_z_pay,y=avg_w_cnt)) +
  geom_smooth(aes(x=avg_z_pay,y=avg_w_cnt),method="lm")
```

```{r}
teams_anl %>% lm(avg_w_cnt ~ avg_z_pay,.) %>% summary()
```

```{r}
cor(teams_anl$avg_z_pay,teams_anl$avg_w_cnt)
```

Use both the plot and the correlation statistics to evaluate (in words) the form (does the relationship look linear?) and strength of the association between these two variables.

> The form looks as if it can be approximated by a linear
model. The correlation is high (cor = 0.6908895, R-squared = 0.4773283)
but significant (p-value: 2.375e-05).  

Would you be comfortable using a linear model to predict the mean number of wins in a given season given their average relative payroll for that season?

> Given such a low R-squared (not close to 1), I'd be hesitant to
predict the exact number of wins, though with such a
low p-value, I might be comfortable making more general
predictions, like predicting the mean number of wins is
within some range etc.

Regression model
----------------

Build a simple linear regression model predicting mean wins from mean payroll z-scores across seasons.

```{r}
teams_anl_lm <- teams_anl %>% lm(avg_w_cnt ~ avg_z_pay,.)
teams_anl_lm %>% anova()
teams_anl_lm %>% summary()
teams_anl_lm %>% glance()
```

What are the total, model, and residual sums of squares for this simple linear regression?

> model: 506.05  
  residual: 554.13

What percent of the variation in mean wins is "explained" by variation in mean payroll z-scores?

> 47.73283% (R-squared)

Write up a summary of your findings.

> The results indicate that payroll predicted wins
(b1 = 5.0438), with 47.73283% of the variance in wins
accounted for by payroll levels. Each standard
deviation payroll was from the annual mean was 
associated with an increase of 5.0434 wins. The OLS
regression equation for predicting wins is of the form

$$
  \textrm{wins}_i = 81.1713 + 5.0438\textrm{standard deviations from mean payroll}_i + \epsilon_i
$$

What is the average of all ŷ i values (in any simple linear regression model) equal to? 

> The mean response (intercept).

What is the variance of the residuals in your regression model?

> 19.107

The standard error?

> 4.449

Compare the variance of the residuals to sample variance of mean wins overall, and to your model R2.

> variance of residuals: 19.79
  variance of mean wins overall: 36.55798
  R-squared: 0.4773
  Adj. R-squared = 0.4587
  
$$
  1-\frac{19.79}{36.55798}\approx 0.4587
$$

How are these three statistics related (in any simple linear regression model)?

$$
  1-\frac{\textrm{variance of residuals}}{\textrm{variance of mean wins overall}}=\textrm{Adj. R-squared}
$$

Obviously, the book and movie about the Oakland A's suggests that this team may be an outlier in terms of the predicting wins from payroll. Look specifically at this team: 

What is the observed number of mean wins?

```{r}
teams_anl %>% filter(teamID == "OAK")
```

> 88.2

What is the predicted?

$$
  81.1713 + 5.0438\times -0.7910688\approx 77.18
$$

What is the residual?

$$
  88.2 - 77.18 = 11.02
$$

How many standard deviations above/below the residual mean is the Oakland A's residual value?

```{r}
augment(teams_anl_lm) %>% filter(avg_w_cnt == 88.2)
```

> 2.56 above

Are there any other teams with a residual value as extreme or more extreme than the Oakland A's?

```{r}
augment(teams_anl_lm) %>% arrange(.std.resid) %>% tail()
```

> No. The next highest is SLN (St. Louis Cardinals) with 1.92

Create a bootstrap distribution for the correlation and the regression coefficients. Copy and paste the following code into your file, and annotate each line with a # to (briefly) explain what each line of code is doing.

```{r}
orig_cor <- 0.6908895
orig_slp <- 5.0438
gt_cor_cnt <- 0
gt_slp_cnt <- 0
N <- 10^4 # storing 10000 as N
cor.boot <- numeric(N) # store vector of size 10000 as cor.boot
int.boot <- numeric(N) # store vector of size 10000 as int.boot
slope.boot <- numeric(N) # store vector of size 10000 as slope.boot
n <- 30 # number of observations here
for (i in 1:N){ # loop 10000 times, storing loop iteration as i
    index <- sample(n, replace = TRUE) # store a vector of size n 
                                       # with values ranging from 1
                                       # to n as index
    team.boot <- teams_anl[index, ] # resampled data
    cor.boot[i] <- cor(team.boot$avg_z_pay, team.boot$avg_w_cnt) 
    # what is x and y? The input & response variables, avg_z_pay and avg_w_cnt
    if(cor.boot[i] > orig_cor){
      gt_cor_cnt <- gt_cor_cnt + 1
    }
    # recalculate linear model estimates
    team.boot.lm <- lm(avg_w_cnt ~ avg_z_pay, data = team.boot) 
    # what is x and y?
    int.boot[i] <- coef(team.boot.lm)[1] # new intercept
    slope.boot[i] <- coef(team.boot.lm)[2] # new slope
    if(slope.boot[i] > orig_slp){
      gt_slp_cnt <- gt_slp_cnt + 1
    }
  }

mean(cor.boot) #mean correlation of bootstrapped data
sd(cor.boot) #standard deviation of correlation of bootstrapped data
quantile(cor.boot, c(.025, .975)) #95% CI of correlation of bootstrapped data

hist(cor.boot) 
#create histogram of correlation of bootstrapped data
observed <- cor(teams_anl$avg_z_pay, teams_anl$avg_w_cnt) 
# what is x and y? The input & response variables avg_z_pay and avg_w_cnt
abline(v = observed, col = "red") # add line at original sample correlation
# do the same as above for slope.boot (don't worry about int.boot)
mean(slope.boot) #mean slope of bootstrapped data
sd(slope.boot) #standard deviation of slope of bootstrapped data
quantile(slope.boot, c(.025, .975)) #95% CI of slope of bootstrapped data

hist(slope.boot) #create histogram of slope of bootstrapped data
observed <- summary(teams_anl_lm)$coefficients[2] 
# what is x and y? The input & response variables avg_z_pay and avg_w_cnt
abline(v = observed, col = "red") # add line at original sample slope

gt_cor_cnt
gt_slp_cnt
```

Figure out how many bootstrap samples had a higher correlation than the one you observed as your original sample correlation.

> 5198 (51.98%)

How many bootstrap samples had a higher slope coefficient than the one you observed.

> 5065 (50.65%)

Use dplyr::mutate() with ifelse() to create a categorical variable that splits our yearID variable into two time intervals: 2000 - 2006 and 2007 - 2014. Then look at your work for question 14 and update to re-calculate average wins and average payroll z-scores separately for each team and time interval (hint: that means two variables in a dplyr::group_by() statement).

```{r}
teams_bat_year_split <- teams_bat %>% # rename these dataframes as appropriate
  mutate(recent = ifelse(yearID < 2007, 0, 1))# rename variables as appropriate
table(teams_bat_year_split$recent, teams_bat_year_split$yearID) # trust but verify
```

```{r}
teams_bat_year_split_anl <- teams_bat_year_split %>%
  group_by(teamID,recent) %>%
  summarise(avg_z_pay = mean(z_pay,na.rm=TRUE),
  avg_w_cnt = mean(W,na.rm=TRUE))
```

```{r}
teams_bat_year_split_anl %>% group_by(teamID) %>%
  ggplot() +
  geom_point(aes(x=avg_z_pay,y=avg_w_cnt)) +
  geom_smooth(aes(x=avg_z_pay,y=avg_w_cnt),method="lm")
```

```{r}
teams_bat_year_split_anl %>% group_by(teamID) %>%
  ggplot() +
  facet_wrap(~ recent) +
  geom_point(aes(x=avg_z_pay,y=avg_w_cnt)) +
  geom_smooth(aes(x=avg_z_pay,y=avg_w_cnt),method="lm")
```

Using ggplot2, create one plot, with side-by-side scatterplots for each time interval, showing mean payroll (x-axis) and mean number of wins (y-axis) across all teams.

```{r}
teams_bat_year_split_anl_w_mean_payroll <- teams_bat_year_split %>%
  group_by(teamID,recent) %>%
  summarise(mean_payroll = mean(payroll,na.rm=TRUE),
  avg_w_cnt = mean(W,na.rm=TRUE))
```

```{r}
teams_bat_year_split_anl_w_mean_payroll %>% group_by(teamID) %>%
  ggplot() +
  geom_point(aes(x=mean_payroll/1000,y=avg_w_cnt)) +
  geom_smooth(aes(x=mean_payroll/1000,y=avg_w_cnt),method="lm")
```

```{r}
teams_bat_year_split_anl_w_mean_payroll %>% group_by(teamID,recent) %>%
  ggplot() +
  facet_wrap(~ recent) +
  geom_point(aes(x=mean_payroll/1000,y=avg_w_cnt)) +
  geom_smooth(aes(x=mean_payroll/1000,y=avg_w_cnt),method="lm")
```

Comment on differences you see between these two plots, and compare to your previous scatterplot across all seasons.

> It appears the slope of the regression line was steeper before 2007, indicating wins were cheaper. The scatterplot across all seasons hides the fact that these two quite models exist and simply draws a regression line over all the datapoints.

Now, run two linear regression analyses (as shown in class), one for each time interval, using dplyr::group_by() %>% do() and broom::tidy()/glance()/augment(). 

```{r}
models <- teams_bat_year_split_anl %>%
  group_by(recent) %>%
  do(mod = lm(avg_w_cnt ~ avg_z_pay, data = .))

models %>% tidy(mod) #coefs

models %>% augment(mod) %>%
  group_by(recent) %>%
  summarize(tot_ss = sum((avg_w_cnt - mean(avg_w_cnt))^2),
            res_ss = sum((avg_w_cnt - .fitted)^2))

models %>% glance(mod)
```

Compare the coefficient estimates to each other, and to your original model. 

> The intercepts of both models are similar (81.14 and 81.18) but the
slopes are not (6.22 and 4.21). The coefficients of the original model
are between these two models' coefficients (original intercept = 81.17
and original slope = 5.04).

Take the Oakland A's team as a specific case: 

Which of your three model/time interval regression models (model 1: across all seasons; model 2: 2000 - 2006; model 3: 2007 - 2014) was better at predicting mean wins for them specifically?
 
```{r}
#Oakland A's actual data
avg_w_cnt = 88.2
avg_z_pay = -0.7910688

#model 1
(81.17 + 5.04 * avg_z_pay) - avg_w_cnt

#model 2
(81.14 + 6.22 * avg_z_pay) - avg_w_cnt

#model 3
(81.18 + 4.21 * avg_z_pay) - avg_w_cnt
```

> model 3  

Which model overall accounted for the most variability in mean wins overall across all teams?

> model 1. It had an R-squared of 0.4773 while model 2 and 3 had
R-squared values of 0.3567 and 0.3947, respectively.

How is the R2 estimate related to the plain old correlation between average wins and average payroll z-scores for each time interval?

$$
\begin{aligned}
  0.4773 = 0.690869^{2} \\
  0.3567 = 0.5972437^{2} \\
  0.3947 = 0.6282515^{2} \\
\end{aligned}
$$

And in general in any simple linear regression model?

> R-squared is the square of the correlation.  

Midterm: Exercises
==================

Part 1: Probability
-------------------

The following table shows the cumulative distribution
function of a discrete random variable. Find the
probability mass function.

```{r}
k = c(0,1,2,3,4,5)
F.k = c(0,.1,.3,.7,.8,1.0)
df = data.frame(k,F.k)
kable(df)
```

$$
\begin{aligned}
  \textrm{cdf: }F(x) = \sum\limits_{t\le x}f(t) \\
  \textrm{pmf: }f(t) = F(x) - F(x-1) = \sum\limits_{t\le x}f(t) - \sum\limits_{t\le x-1}f(t) \\
  \\ \hline \\
  f(0) = F(0) = 0 \\
  f(1) = F(1) - F(0) = .1 - 0 = .1 \\
  f(2) = F(2) - F(1) = .3 - .1 = .2 \\
  f(3) = F(3) - F(2) = .7 - .3 = .4 \\
  f(4) = F(4) - F(3) = .8 - .7 = .1 \\
  f(5) = F(5) - F(4) = 1.0 - .8 = .2 \\
\end{aligned}
$$

```{r}
k = c(0,1,2,3,4,5)
F.k = c(0,.1,.3,.7,.8,1.0)
f.k = c(0,.1,.2,.4,.1,.2)
df = data.frame(k,F.k,f.k)
kable(df)
```

The probability density function of a random variable X is given by:

$$
  f(x)=\begin{cases}
    cx, & \textrm{$0<x<4$} \\
    0, & \text{otherwise} \\
  \end{cases}
$$

a) find c

$$
\begin{aligned}
  \int_{-\infty}^{\infty} f_x(x)\,dx = 1 \\
\end{aligned}
$$
$$
\begin{aligned}
  \\ \hline \\
  1 = \int_{0}^{4} cx\,dx \\
  = c\times \Big[\frac{x^2}{2}\Big]_0^4 \\
  = c\times \Big[\frac{4^2}{2} - 0\Big] \\
  = c\times \Big[\frac{16}{2}\Big] \\
  = 8c \\
\end{aligned}
$$
$$
\begin{aligned}
  \\ \hline \\
  c = \frac{1}{8} \\
\end{aligned}
$$

b) find the cumulative distribution function F(x).

$$
\begin{aligned}
  F_x(x) = \int_{-\infty}^{x} f_x(y)\,dy = P(X\le x) \\
  \\ \hline \\
  F_x(x) = \int_{0}^{x} \frac{1}{8}(y)\,dy\, \textrm{when $0\le x\le 4$} \\
  = \frac{1}{8} \Big[\frac{y^2}{2}\Big]_0^{x} \\
  = \frac{1}{8} \Big[\frac{x^2}{2} - 0\Big] \\
  = \frac{x^2}{16} \\
\end{aligned}
$$
$$
\begin{aligned}
  \\ \hline \\
  F_x(x)=\begin{cases}
    0, & \textrm{$x<0$} \\
    \frac{x^2}{16}, & \textrm{$0\le x\le 4$} \\
    1, & \textrm{$x>4$} \\
  \end{cases}
\end{aligned}
$$

c) Compute P(1 < X < 3)

$$
\begin{aligned}
  P(1 < X < 3) = F_x(3) - F_x(1) \\
  = \frac{3^2}{16} - \frac{1^2}{16} \\
  = \frac{9}{16} - \frac{1}{16} \\
  = \frac{8}{16} \\
  = \frac{1}{2} \\
  = 0.5 \\
\end{aligned}
$$

The random variable X has a cumulative distribution function 
(cdf):

$$
\begin{aligned}
  F(x)=\begin{cases}
    0, & \textrm{$x\le 0$} \\
    \frac{x^3}{2+x^2}, & \textrm{$x>0$} \\
  \end{cases}
\end{aligned}
$$

Find the probability density function (pdf) of X.

$$
\begin{aligned}
  pdf(x) = f_x(x) = F'(x) \\
  = \frac{x^3}{2+x^2}'\, \textrm{when $x>0$} \\
  = \frac{(2 + x^2)\times 3x^2 - x^3\times 2x}{(2 + x^2)^2}\, \textrm{(quotient rule)} \\
  = \frac{6x^2 + 3x^4 - 2x^4}{(2 + x^2)^2} \\
  = \frac{6x^2 + x^4}{(2 + x^2)^2} \\
  = \frac{x^2(6+x^2)}{(2 + x^2)^2} \\
  \\ \hline \\
  f_x(x)=\begin{cases}
    0, & \textrm{$x\le 0$} \\
    \frac{x^2(6+x^2)}{(2 + x^2)^2}, & \textrm{$x>0$} \\
  \end{cases}
\end{aligned}
$$

The joint probability of the continuous random variable
(X, Y) is given by:

$$
\begin{aligned}
  f(x,y)=\begin{cases}
    \frac{1}{28}(4x + 2y + 1), & \textrm{$0\le x<2,\,0\le y <2$} \\
    0, & \textrm{otherwise} \\
  \end{cases}
\end{aligned}
$$

Find E(XY)

$$
\begin{aligned}
  E\big[XY\big] = \int_{0}^{2} \int_{0}^{2} xy\Big[ \frac{1}{28}(4x + 2y + 1)\Big]\,dxdy \\
  = \frac{1}{28}\int_{0}^{2} \int_{0}^{2} 4x^2y + 2xy^2 + xy\,dxdy \\
\end{aligned}
$$
$$
\begin{aligned}
  \\ \hline \\
  \textrm{(expr. 1)} = \frac{1}{28} \int_{0}^{2} 4x^2y + 2xy^2 + xy\,dx \\
  = \frac{1}{28} \Big[4\frac{x^3}{3}y + 2\frac{x^2}{2}y^2 + \frac{x^2}{2}y \Big]_{x=0}^{x=2} \\
  = \frac{1}{28} \Big[4\frac{8}{3}y + 2\times 2y^2 + 2y \Big] \\
  = \frac{1}{28} \Big[\frac{32y}{3} + 4y^2 + 2y \Big] \\
  = \frac{1}{28} \Big[\frac{38y}{3} + 4y^2 \Big] \\
  = \frac{38y}{84} + \frac{4y^2}{28} \\
  = \frac{19y}{42} + \frac{y^2}{7} \\
\end{aligned}
$$
$$
\begin{aligned}
  \\ \hline \\
  E\big[XY\big] = \int_{0}^{2} \textrm{(expr. 1)}\,dy \\
  = \int_{0}^{2} \frac{19y}{42} + \frac{y^2}{7}\,dy \\
  = \Big[\frac{19}{42}\times \frac{y^2}{2}+ \frac{1}{7}\times \frac{y^3}{3}\Big]_0^2 \\
  = \Big[\frac{19}{42}\times 2 + \frac{1}{7}\times \frac{8}{3}\Big]-0 \\
  \approx 1.286 \\
\end{aligned}
$$

Find Cov(X, Y)

$$
\begin{aligned}
  \textrm{Marginal Probabilities:} \\
  \\ \hline \\
  f_x(x) = \int_{-\infty}^{\infty} f(x,y) dy \\
  f_y(y) = \int_{-\infty}^{\infty} f(x,y) dx \\
\end{aligned}
$$

$$
\begin{aligned}
  \\ \hline \\
  f_x(x) = \int_{0}^{2} \frac{1}{28}(4x + 2y + 1)\,dy \\
  =\frac{1}{28}\Big[ 4xy+2\frac{y^2}{2}+y\Big]_0^2 \\
  =\frac{1}{28}\Big[ 8x + 4 + 2\Big] \\
  =\frac{8x}{28} + \frac{6}{28} \\
  =\frac{2x}{y} + \frac{3}{14} \\
\end{aligned}
$$

$$
\begin{aligned}
  \\ \hline \\
  f_y(y) = \int_{0}^{2} \frac{1}{28}(4x + 2y + 1)\,dx \\
  =\frac{1}{28}\Big[4\frac{x^2}{2} + 2yx + x\Big]_0^2 \\
  =\frac{1}{28}\Big[8+4y+2\Big] \\
  =\frac{4y}{28} + \frac{10}{28} \\
  =\frac{y}{7} + \frac{5}{14} \\
\end{aligned}
$$

$$
\begin{aligned}
  \\ \hline \\
  E\big[X\big] = \int_{-\infty}^{infty} xf_x(x)\,dx \\
  =\int_{0}^{2} x\Big[\frac{2x}{7} + frac{3}{14}\Big]\,dx \\
  =\int_{0}^{2} \frac{2x^2}{7}+ \frac{3x}{14}\,dx \\
  =\Big[\frac{2x^3}{21} + \frac{3x^2}{28}\Big]_0^2 \\
  =\Big[\frac{16}{21} + \frac{12}{28}\Big] - 0 \\
  \approx 1.190 \\
\end{aligned}
$$
$$
\begin{aligned}
  \\ \hline \\
  E\big[Y\big] = \int_{-\infty}^{infty} yf_y(y)\,dy \\
  =\int_{0}^{2} y\Big[\frac{y}{7}+ \frac{5}{14}\Big]\,dy \\
  =\int_{0}^{2} \frac{y^2}{7}+ \frac{5y}{14}\,dy \\
  =\Big[\frac{y^3}{21}+\frac{5y^2}{28}\Big]_0^2 \\
  =\Big[\frac{8}{21} + \frac{20}{28}\Big] - 0 \\
  \approx 1.095 \\
\end{aligned}
$$
$$
\begin{aligned}
  \\ \hline \\
  \textrm{Cov(X,Y) = E[XY] - E[X]E[Y]} \\
  \approx 1.286 - 1.095\times 1.190 \\
  \approx 1.286 - 1.30305 \\
  \approx -0.01705
\end{aligned}
$$

Find the correlation coefficient $\rho_{XY}$

$$
\begin{aligned}
  Var(X) = \int_{-\infty}^{\infty} (X - \mu x)^2 f(x)\,dx \\
  =\int_0^2 (X-1.190)^2 \times \Big[\frac{2x}{7} + \frac{3}{14}\Big]\,dx \\
  \textrm{used wolfram alpha solver} \\
  \approx 0.297 \\
\end{aligned}
$$

$$
\begin{aligned}
  Var(Y) = \int_{-\infty}^{\infty} (Y - \mu y)^2 f(y)\,dy \\
  =\int_0^2 (Y-1.095)^2 \times \Big[\frac{y}{7} + \frac{5}{14}\Big]\,dy \\
  \textrm{used wolfram alpha solver} \\
  \approx 0.324 \\
\end{aligned}
$$

$$
\begin{aligned}
  Cor(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)\times Var(Y)}} \\
  \approx \frac{-0.017}{\sqrt{0.297\times 0.324}} \\
  \approx -0.055
\end{aligned}
$$

Part 2: Sampling Distributions
------------------------------

Examine the behavior of a lognormal random variable with
parameters 0.2938933 and 1.268636.

```{r}
set.seed(12345)
logn_1samp <- rlnorm(1e+07, 0.2938933, 1.268636)
mean(logn_1samp)
sd(logn_1samp)
```

Transform this variable linearly so that we have a new
variable Y mean of 100 and a standard deviation of 15.

```{r}
set.seed(12345)
logn_1samp <- 2.5 * rlnorm(1e+07, 0.2938933, 1.268636) + 92.5
mean(logn_1samp)
sd(logn_1samp)
```

Take 100,000 means based on samples of size 25 from the
transformed lognormal distribution.

```{r}
set.seed(12345)
N = 100000
logn_means <- numeric(N)
for (i in 1:N) {
    x <- 2.5 * rlnorm(25, 0.2938933, 1.268636) + 92.5
    logn_means[i] <- mean(x)
}
head(logn_means)
```

Examine the population, sample, and sampling distributions.

```{r}
logn_means %>% hist(breaks=100,main="Smp. Dist. of logn Means")
```

```{r}
plot(density(logn_means),main="Smp. Dist. of logn Means")
```

What did you expect to see?

> I suppose I expected the means to be evenly distributed
above and below 100.

What do you actually see?

> A high peak around 100 with a long tail to the right.

What is the mean/standard deviation of this simulated 
sampling distribution?

```{r}
mean(logn_means)
sd(logn_means)
```

> mean: 100.0029
  sd: 3.023481, odd. didn't we transform this to 15?

Do the same for an exponential distribution with mean
and standard deviation of 1.

```{r}
set.seed(12345)
exp_1samp <- rexp(1e+07,1)
mean(exp_1samp)
sd(exp_1samp)
```

```{r}
set.seed(12345)
exp_1samp <- 15 * rexp(1e+07,1) + 92.5
mean(exp_1samp)
sd(exp_1samp)
```

```{r}
set.seed(12345)
N = 100000
exp_means <- numeric(N)
for (i in 1:N) {
    x <- 15 * rexp(25, 1) + 85 #transform so mean = 100, sd = 15
    exp_means[i] <- mean(x)
}
head(exp_means)
```

```{r}
exp_means %>% hist(breaks=100,main="Smp. Dist. of exp Means")
```

```{r}
plot(density(exp_means),main="Smp. Dist. of exp Means")
```

```{r}
mean(exp_means)
sd(exp_means)
```

> This distribution has a longer tail on the right than
on the left but is not as extreme as the lognormal
distribution. The sd looks like it's near 3, just like the
lognormal distribution above, though it was transformed.

Overall, what conclusions do you make about the applicability of the Central Limit Theorem given what we have demonstrated with variables from: 

The binomial distribution (in class).
The normal distribution (warm-up).
The uniform distribution (warm-up).
The lognormal (on your own).
The exponential (on your own).

> With enough iterations, sample distributions for
each of the above distributions converge to the same
distribution: the normal distribution.

Part 3: Problems from your peers!
---------------------------------

You attend a party where there are already 20 guests in the room. 
Unbeknownst to you, 5 guests are zombies, and 7 are vampires.

One person approaches you and buys you a drink. What is the probability
that this person is a vampire?

> Assuming a vampire is equally as likely approach me and
buy me a drink as any other guest, (Probably a naive assumption)

$$
\begin{aligned}
 P(V)=\frac{7}{20} \\
 =0.35 \\
\end{aligned}
$$

Two people approach you and ask your opinion on the host’s outfit. What
is the probability that they are both zombies?

> Assuming a zombie is equally as likely to approach me and
ask my opinion on the host's outfit as any other guest and
that these two events are independent, (Again, probably
a naive assumption; it is a well known fact that zombies
perform a complex flocking phenomenon.)

$$
\begin{aligned}
 P(Z_1)=\frac{5}{20}=0.25 \\
 P(Z_2)=\frac{4}{19}\approx 0.21 \\
 P(Z_1)\cap P(Z_2)= P(Z_1)P(Z_2)\textrm{ from Def. 2.4.2 (p. 73)} \\
 \approx 0.25\times 0.21 \\
 \approx 0.0525 \\
\end{aligned}
$$

Three people approach you and ask you to be the fourth player in their 
Texas hold’em game. What is the probability that they are all normal
humans?

> Assuming all the guests at the party who are not vampires
or zombies are normal humans and assuming the aforementioned
bits about equal likelyhood and independence,

$$
\begin{aligned}
 P(H_1)=\frac{8}{20}=0.4 \\
 P(H_2)=\frac{7}{19}\approx 0.368 \\
 P(H_3)=\frac{6}{18}\approx 0.333 \\
 P(H_1)\cap P(H_2)\cap P(H_3)= P(H_1)P(H_2)P(H_3)\textrm{ from Def. 2.4.2 (p. 73)} \\
 \approx 0.4\times 0.368\times 0.333 \\
 \approx 0.0490176 \\
\end{aligned}
$$

Bud only goes out trick-or-treating when there are clear skies (not too
dark or too wet) and there is no full moon (he’s superstitious). There
is a full moon every 27.32 days. Assuming it is a random Halloween –
i.e. we are not aware of any weather forecast or pattern, nor recent
moon phases – and the probability of clear skies on a random October
31 is 0.6, what is the probability that Bud will go trick-or-treating?

> M = full moon  
  C = clear skies  
  T = Bud goes out trick-or-treating

$$
\begin{aligned}
   P(M) = \frac{1}{27.32}\approx 0.037 \\
   P(C) = .06\textrm{(given)} \\
   P(T) = P(C)P(M^{c}) \\
\end{aligned}
$$

$$
  P(M^{c}) = 1-P(M)\textrm{ from Property 1 (p. 58)}
$$

$$
\begin{aligned}
   P(T) = P(C)(1-P(M)) \\
   P(T) \approx 0.6\times (1-0.037) \\
   P(T) \approx 0.6\times 0.963 \\
   \approx 0.5778 \\
\end{aligned}
$$
